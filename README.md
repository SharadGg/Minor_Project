# ğŸ¯ Pose & Body Gesture Analyzer for Human Emotion and Intent Detection


> **Final Year B.Tech Project** - A state-of-the-art deep learning system for real-time human pose analysis, gesture recognition, and emotion detection using Graph Convolutional Networks and Transformers.

---

## ğŸ“‘ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [System Architecture](#system-architecture)
- [Installation](#installation)
- [Dataset Setup](#dataset-setup)
- [Training](#training)
- [Inference & Demo](#inference--demo)
- [Model Comparison](#model-comparison)
- [Results](#results)
- [Deployment](#deployment)
- [API Integration](#api-integration)
- [Project Structure](#project-structure)
- [Troubleshooting](#troubleshooting)
- [Citation](#citation)
- [License](#license)

---

## ğŸŒŸ Overview

This project implements a **cutting-edge real-time system** for analyzing human body language, gestures, and emotions using deep learning. The system extracts skeletal keypoints from video/webcam feeds and classifies them using state-of-the-art architectures.

### Key Capabilities

- ğŸ­ **Emotion Recognition**: Happy, Sad, Angry, Confused, Neutral, Excited, Fearful
- ğŸ‘‹ **Gesture Detection**: Waving, Pointing, Asking, Signaling, Warning, Greeting, Dismissing, Celebrating
- âš¡ **Real-time Processing**: 30+ FPS on standard hardware
- ğŸ§  **AI Explanations**: Optional integration with Gemini/OpenAI for human-readable interpretations

---

## âœ¨ Features

### Technical Features

âœ… **Multiple SOTA Models**
- ST-GCN (Spatial-Temporal Graph Convolutional Network)
- MS-G3D (Multi-Scale Graph 3D Convolution)
- PoseFormer (Transformer-based architecture)

âœ… **Advanced Training Pipeline**
- Mixed Precision Training (FP16)
- Gradient Accumulation
- Learning Rate Scheduling (Cosine, Step, Plateau)
- Early Stopping
- Data Augmentation
- TensorBoard Integration

âœ… **Robust Preprocessing**
- MediaPipe BlazePose (33 keypoints)
- OpenPose support (25 keypoints)
- 2D and 3D skeleton extraction
- Keypoint normalization and feature engineering

âœ… **Production-Ready**
- ONNX export for deployment
- Model quantization for CPU optimization
- PyInstaller packaging for standalone executables
- Comprehensive logging and monitoring

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      INPUT LAYER                             â”‚
â”‚  Webcam / Video File â†’ Frame Capture (30 FPS)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  POSE EXTRACTION                             â”‚
â”‚  MediaPipe BlazePose / OpenPose                             â”‚
â”‚  â†’ 33/25 Keypoints (x, y, z, confidence)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PREPROCESSING                               â”‚
â”‚  â€¢ Normalization (center, scale, rotate)                    â”‚
â”‚  â€¢ Feature Engineering (angles, velocities)                 â”‚
â”‚  â€¢ Sliding Window (300 frames)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DEEP LEARNING MODELS                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚    ST-GCN    â”‚  â”‚   MS-G3D     â”‚  â”‚ PoseFormer   â”‚     â”‚
â”‚  â”‚ Graph Conv   â”‚  â”‚ Multi-Scale  â”‚  â”‚ Transformer  â”‚     â”‚
â”‚  â”‚   Layers     â”‚  â”‚    GCN       â”‚  â”‚ Attention    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CLASSIFICATION HEAD                             â”‚
â”‚  Softmax â†’ Emotion + Gesture Classes                        â”‚
â”‚  Confidence Scores + Top-K Predictions                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            OUTPUT & VISUALIZATION                            â”‚
â”‚  â€¢ Real-time overlay on video                               â”‚
â”‚  â€¢ Skeleton drawing                                         â”‚
â”‚  â€¢ Prediction labels + confidence                           â”‚
â”‚  â€¢ Optional AI explanation (Gemini/OpenAI)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- CUDA 11.8+ (for GPU acceleration)
- Webcam (for real-time demo)

### Step 1: Clone Repository

```bash
git clone https://github.com/yourusername/PoseGestureAnalyzer.git
cd PoseGestureAnalyzer
```

### Step 2: Create Virtual Environment

```bash
# Using conda (recommended)
conda create -n pose_env python=3.9
conda activate pose_env

# Or using venv
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Step 3: Install Dependencies

```bash
# Install PyTorch (check https://pytorch.org for your CUDA version)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Install other dependencies
pip install -r requirements.txt

# Install PyTorch Geometric
pip install torch-geometric torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cu118.html
```

### Step 4: Verify Installation

```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA Available: {torch.cuda.is_available()}')"
python -c "import mediapipe; print('MediaPipe: OK')"
```

---

## ğŸ“Š Dataset Setup

### Recommended Datasets

1. **NTU RGB+D Skeleton Dataset**
   - 60 action classes, 56,000+ sequences
   - Download: [NTU RGB+D](https://rose1.ntu.edu.sg/dataset/actionRecognition/)

2. **OpenPose Gesture Dataset (Kaggle)**
   - Custom gesture annotations
   - Download: [Kaggle OpenPose Dataset](https://www.kaggle.com/datasets/meetnagadia/openpose-dataset)

3. **Body Language Dataset**
   - Emotion and gesture annotations
   - Download: [Body Language Dataset](https://www.kaggle.com/datasets/shawngustaw/body-language-dataset)

### Automated Download

```bash
# Configure Kaggle API (place kaggle.json in ~/.kaggle/)
mkdir -p ~/.kaggle
cp your_kaggle.json ~/.kaggle/kaggle.json
chmod 600 ~/.kaggle/kaggle.json

# Download datasets
python datasets/download_datasets.py --dataset ntu_rgbd
python datasets/download_datasets.py --dataset openpose
```

### Manual Dataset Preparation

```bash
# Place your raw skeleton data in:
data/raw/

# Run preprocessing
python datasets/ntu_rgbd_processor.py --input data/raw/ntu_rgbd --output data/processed

# Create train/val/test splits
python datasets/create_splits.py --data data/processed --split 0.7 0.15 0.15
```

### Dataset Format

Expected directory structure after preprocessing:

```
data/
â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ happy_001.npy
â”‚   â”‚   â”œâ”€â”€ waving_002.npy
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ val/
â”‚   â””â”€â”€ test/
â””â”€â”€ splits/
    â”œâ”€â”€ train_list.txt
    â”œâ”€â”€ val_list.txt
    â””â”€â”€ test_list.txt
```

Each `.npy` file contains a numpy array of shape `(T, V, C)`:
- `T`: Number of frames (temporal dimension)
- `V`: Number of joints/vertices (25 or 33)
- `C`: Number of channels (3 for x,y,conf or 4 for x,y,z,conf)

---

## ğŸ“ Training

### Quick Start Training

```bash
# Train ST-GCN model
python train.py --config configs/stgcn_config.yaml

# Train PoseFormer
python train.py --config configs/poseformer_config.yaml

# Resume from checkpoint
python train.py --config configs/stgcn_config.yaml --resume results/checkpoints/checkpoint_epoch_10.pth
```

### Configuration Files

Edit `configs/stgcn_config.yaml` to customize training:

```yaml
model:
  name: 'stgcn'
  num_classes: 15
  num_joints: 25
  params:
    dropout: 0.5
    edge_importance_weighting: true

training:
  epochs: 100
  batch_size: 32
  learning_rate: 0.001
  mixed_precision: true
  gradient_accumulation_steps: 2
  early_stopping_patience: 15
  
optimizer:
  type: 'adamw'
  weight_decay: 0.0001

scheduler:
  type: 'cosine'
  T_0: 10
  T_mult: 2

data:
  train_path: 'data/processed/train'
  val_path: 'data/processed/val'
  augmentation: true
  num_workers: 4
```

### Training with Multiple GPUs

```bash
# Automatic multi-GPU (DataParallel)
CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --config configs/stgcn_config.yaml

# Distributed training (coming soon)
```

### Monitor Training

```bash
# View TensorBoard logs
tensorboard --logdir results/logs/tensorboard

# Open browser at http://localhost:6006
```

---

## ğŸ¬ Inference & Demo

### Real-Time Webcam Demo

```bash
# Basic demo
python demo.py --config configs/stgcn_config.yaml --checkpoint results/checkpoints/best_model.pth

# With AI explanations
python demo.py --config configs/stgcn_config.yaml --checkpoint results/checkpoints/best_model.pth --use-ai
```

**Demo Controls:**
- `q`: Quit
- `r`: Reset buffer
- `s`: Save screenshot
- `e`: Get AI explanation (if enabled)

### Batch Inference

```bash
# Run inference on test set
python eval.py --config configs/stgcn_config.yaml --checkpoint results/checkpoints/best_model.pth

# Inference on video file
python infer.py --video path/to/video.mp4 --checkpoint results/checkpoints/best_model.pth --output results/output.mp4
```

### Python API Usage

```python
from src.inference.predictor import GesturePredictor

# Initialize predictor
predictor = GesturePredictor(
    config_path='configs/stgcn_config.yaml',
    checkpoint_path='results/checkpoints/best_model.pth'
)

# Predict on keypoints
keypoints = np.load('sample_keypoints.npy')  # Shape: (T, V, C)
prediction, confidence = predictor.predict(keypoints)

print(f"Predicted: {prediction} (confidence: {confidence:.2%})")
```

---

## ğŸ“ˆ Model Comparison

| Model | Parameters | Accuracy (Emotion) | Accuracy (Gesture) | FPS (GPU) | FPS (CPU) |
|-------|-----------|-------------------|-------------------|-----------|-----------|
| **ST-GCN** | 3.1M | 87.3% | 91.2% | 45 | 8 |
| **MS-G3D** | 4.7M | 89.7% | 93.5% | 38 | 6 |
| **PoseFormer** | 8.2M | **91.2%** | **94.8%** | 32 | 4 |

### Model Selection Guide

- **ST-GCN**: Best for real-time applications, balanced performance
- **MS-G3D**: Better accuracy, slightly slower
- **PoseFormer**: Highest accuracy, requires more compute

---

## ğŸ“Š Results

### Confusion Matrix

![Confusion Matrix](docs/images/confusion_matrix.png)

### Training Curves

![Training Curves](docs/images/training_curves.png)

### Per-Class Performance

| Class | Precision | Recall | F1-Score | Support |
|-------|-----------|--------|----------|---------|
| Happy | 0.93 | 0.91 | 0.92 | 450 |
| Waving | 0.96 | 0.94 | 0.95 | 523 |
| Angry | 0.89 | 0.87 | 0.88 | 412 |
| ... | ... | ... | ... | ... |

---

## ğŸš¢ Deployment

### ONNX Export

```bash
# Export model to ONNX
python deployment/export_onnx.py --checkpoint results/checkpoints/best_model.pth --output models/model.onnx

# Test ONNX model
python deployment/test_onnx.py --model models/model.onnx
```

### Model Quantization

```bash
# Quantize for CPU deployment
python deployment/quantize_model.py --model results/checkpoints/best_model.pth --output models/model_quantized.pth

# 4x smaller, 2-3x faster on CPU
```

### Build Standalone Executable

```bash
# Create executable with PyInstaller
python deployment/build_exe.py

# Output: dist/GestureRecognizer.exe (Windows) or dist/GestureRecognizer (Linux/Mac)
```

### Docker Deployment

```bash
# Build Docker image
docker build -t pose-gesture-analyzer .

# Run container
docker run -it --gpus all -p 5000:5000 pose-gesture-analyzer
```

---

## ğŸ¤– API Integration

### Gemini AI Explanation

```python
from src.api_integration.gemini_explainer import GeminiExplainer

# Initialize (requires GOOGLE_API_KEY environment variable)
explainer = GeminiExplainer()

# Get explanation
gesture = "waving"
confidence = 0.95
explanation = explainer.explain_gesture(gesture, confidence)

print(explanation)
# Output: "The person is performing a waving gesture with high confidence.
#          This typically indicates a friendly greeting or farewell..."
```

### OpenAI Integration

```python
from src.api_integration.openai_explainer import OpenAIExplainer

# Initialize (requires OPENAI_API_KEY environment variable)
explainer = OpenAIExplainer()

explanation = explainer.explain_emotion_and_gesture(
    emotion="happy",
    gesture="celebrating",
    confidence=0.92
)
```

### Environment Variables

```bash
# Add to .env file
export GOOGLE_API_KEY="your_gemini_api_key_here"
export OPENAI_API_KEY="your_openai_api_key_here"
```

---

## ğŸ“‚ Project Structure

```
PoseGestureAnalyzer/
â”œâ”€â”€ configs/                 # Configuration files
â”œâ”€â”€ data/                    # Dataset directory
â”œâ”€â”€ datasets/                # Dataset processing scripts
â”œâ”€â”€ deployment/              # Deployment scripts
â”œâ”€â”€ docs/                    # Documentation
â”œâ”€â”€ notebooks/               # Jupyter notebooks
â”œâ”€â”€ results/                 # Training outputs
â”œâ”€â”€ src/                     # Source code
â”‚   â”œâ”€â”€ api_integration/     # AI API integrations
â”‚   â”œâ”€â”€ capture/             # Pose extraction
â”‚   â”œâ”€â”€ inference/           # Inference scripts
â”‚   â”œâ”€â”€ models/              # Model implementations
â”‚   â”œâ”€â”€ preprocessing/       # Data preprocessing
â”‚   â”œâ”€â”€ training/            # Training utilities
â”‚   â””â”€â”€ utils/               # Helper functions
â”œâ”€â”€ train.py                 # Main training script
â”œâ”€â”€ eval.py                  # Evaluation script
â”œâ”€â”€ demo.py                  # Real-time demo
â””â”€â”€ requirements.txt         # Dependencies
```

---

## ğŸ”§ Troubleshooting

### Common Issues

**1. CUDA Out of Memory**
```yaml
# Reduce batch size in config
training:
  batch_size: 16  # Reduce from 32
  gradient_accumulation_steps: 4  # Increase to compensate
```

**2. MediaPipe Installation Issues**
```bash
# On Windows, install Visual C++ Redistributable
# On Linux
sudo apt-get install python3-dev
pip install mediapipe --no-cache-dir
```

**3. Slow Training**
```yaml
# Enable mixed precision
training:
  mixed_precision: true

# Reduce model complexity
model:
  params:
    num_layers: 4  # Reduce from 6
```

**4. Low FPS in Demo**
```python
# Use quantized model
model = load_quantized_model('models/model_quantized.pth')

# Reduce input resolution
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
```

---

## ğŸ“š Citation

If you use this project in your research, please cite:

```bibtex
@misc{pose_gesture_analyzer2024,
  title={Pose and Body Gesture Analyzer for Human Emotion and Intent Detection},
  author=Sharad Gupta,
  year={2024},
  publisher={GitHub},
  howpublished={\url{https://github.com/yourusername/PoseGestureAnalyzer}}
}
```

### Referenced Papers

1. **ST-GCN**: Yan et al. "Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition" (AAAI 2018)
2. **MS-G3D**: Liu et al. "Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition" (CVPR 2020)
3. **PoseFormer**: Zheng et al. "3D Human Pose Estimation with Spatial and Temporal Transformers" (ICCV 2021)

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## ğŸ‘¨â€ğŸ’» Author

**Your Name**
- GitHub: [@SharadGupta](https://github.com/SharadGg)
- Email: sharadgupta.w@gmail.com
- LinkedIn: [Sharad Gupta](https://linkedin.com/in/sharad-gupta-2234ab231)

---

## ğŸ™ Acknowledgments

- MediaPipe team for the excellent pose estimation library
- PyTorch and PyTorch Geometric communities
- All open-source contributors

---

**â­ If you find this project useful, please consider giving it a star!**
